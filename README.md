# ğŸ“Š House Value Prediction â€“ Regression Models

This project focuses on solving a **regression problem** by predicting house values using machine learning.  
Multiple regression models were trained, evaluated, and compared using a consistent preprocessing pipeline and evaluation metrics.

The project also demonstrates **outlier handling at both feature and target levels** and analyzes its impact on different regression models.

---

## ğŸ“Œ Problem Statement
Predict house values based on demographic and housing-related features using regression models and evaluate their performance.

---

## ğŸ› ï¸ Tech Stack
- Python
- Pandas, NumPy
- Scikit-learn
- Matplotlib
- Joblib

---

## ğŸ“‚ Dataset Overview
- Real-world housing dataset
- Target variable: **House Value (continuous)**
- Contains numerical and categorical features
- Includes skewed feature distributions and target outliers

---

## ğŸ”§ Data Preprocessing & Feature Engineering

### 1ï¸âƒ£ Missing Values
- Rows with missing values (~1% of data) were safely removed using `dropna()`.

### 2ï¸âƒ£ Categorical Encoding
- Categorical features were encoded using **OneHotEncoder**
- Implemented via **ColumnTransformer** for pipeline consistency

### 3ï¸âƒ£ Outlier & Skewness Handling

#### ğŸ”¹ Feature-Level Handling
To improve model stability:

- **Log Transformation (`log1p`)**
  - Applied to highly skewed numerical feature columns
  - Reduced right-skewness and stabilized feature distributions

- **IQR-based Capping**
  - Applied to numerical feature outliers
  - Reduced the influence of extreme values

#### ğŸ”¹ Target-Level Handling (Experiment)
- **IQR-based capping was applied to the target variable**
- This experiment was performed to study the sensitivity of different models to target outliers

ğŸ“Œ **Observation:**
- Linear Regression showed a **measurable improvement**
- Decision Tree and Random Forest showed **minimal change**, indicating robustness to target outliers

---

## ğŸ¤– Models Trained

All models were trained using the **same preprocessing pipeline and trainâ€“test split**.

1. **Linear Regression**
   - Highly sensitive to outliers
   - Benefited from target IQR-based capping

2. **Decision Tree Regressor**
   - Captures non-linear relationships
   - Naturally robust to outliers

3. **Random Forest Regressor**
   - Ensemble of decision trees
   - Strong generalization and outlier robustness
   - Best-performing model overall

---

## ğŸ“Š Evaluation Metrics

- **RÂ² Score** â€“ Variance explained by the model
- **Mean Absolute Error (MAE)** â€“ Average absolute prediction error
- **Root Mean Squared Error (RMSE)** â€“ Penalizes large errors

---

## ğŸ“ˆ Model Performance Comparison (After Target IQR Capping)

| Model | RÂ² Score | MAE | RMSE |
|------|----------|-----|------|
| Linear Regression | **0.64** | **51,918** | **68,610** |
| Decision Tree Regressor | 0.73 | 40,353 | 60,276 |
| Random Forest Regressor | **0.83** | **31,596** | **48,674** |

ğŸ“Œ **Key Insight:**  
Target IQR-based capping improved Linear Regression performance, while Decision Tree and Random Forest remained largely unchanged due to their inherent robustness to outliers.

---

## ğŸ“‰ Residual Analysis
- Residual plots were generated for all models
- Random Forest residuals showed:
  - Random scatter around zero
  - No systematic patterns
  - Better error distribution compared to LR and DT

This confirms good generalization and reduced bias.

---

## ğŸ’¾ Model Artifacts & Reproducibility
- Trained model files (`.pkl`) were generated locally
- Due to GitHub file size limits, model artifacts are excluded from version control
- A `.gitignore` file is used to prevent accidental commits of large files

ğŸ“Œ All results are **fully reproducible** by running the notebook.

---

## ğŸ† Conclusion
- Linear Regression benefited from **target-level IQR-based outlier handling**
- Decision Tree and Random Forest were largely unaffected due to robustness
- Random Forest achieved the best overall performance
- Outlier handling experiments provided valuable insights into model sensitivity

---

## ğŸ“Œ Key Learnings
- Linear models are sensitive to target outliers
- Tree-based models are naturally robust
- Outlier handling should be evaluated empirically
- Fair model comparison requires consistent preprocessing

---

## ğŸš€ Future Improvements
- Hyperparameter tuning using GridSearchCV
- Cross-validation
- Feature importance analysis
- Model deployment using Streamlit or FastAPI
